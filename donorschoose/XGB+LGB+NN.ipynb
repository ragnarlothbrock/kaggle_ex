{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Hi everybody,\n",
    "\n",
    "in this notebook, I'm going to present some text and numeric feature extraction techniques. Some of them are already presented in the other kernels and some are new. We will focus mostly on the text and we try to place ourselves in the shoes of grant administrators to see what they might focus on when processing the application, consciously or unconsciously.\n",
    "\n",
    "Let's start with importing modules and loading the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl # linear algebra + plots\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.sparse import hstack\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "Ttr = pd.read_csv('train.csv')\n",
    "Tts = pd.read_csv('test.csv', low_memory=False)\n",
    "R = pd.read_csv('resources.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "We know from the data description page that the essay column formats had changed on 2016-05-17, and thereafter, there are only 2 essays; essay 1 matches to the combination of essays 1&2 and new essay 2 is somehow equal to old essays 3&4.\n",
    "\n",
    "So, I first move the contents of 'project_essay_2' to 'project_essay_4' when essay 4 is nan, then we simply combine 1&2 and 3&4 to make a uniform dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the tables into one\n",
    "target = 'project_is_approved'\n",
    "Ttr['tr'] = 1; Tts['tr'] = 0\n",
    "Ttr['ts'] = 0; Tts['ts'] = 1\n",
    "\n",
    "T = pd.concat((Ttr,Tts))\n",
    "\n",
    "T.loc[T.project_essay_4.isnull(), ['project_essay_4','project_essay_2']] = \\\n",
    "    T.loc[T.project_essay_4.isnull(), ['project_essay_2','project_essay_4']].values\n",
    "\n",
    "T[['project_essay_2','project_essay_3']] = T[['project_essay_2','project_essay_3']].fillna('')\n",
    "\n",
    "T['project_essay_1'] = T.apply(lambda row: ' '.join([str(row['project_essay_1']), \n",
    "                                                     str(row['project_essay_2'])]), axis=1)\n",
    "T['project_essay_2'] = T.apply(lambda row: ' '.join([str(row['project_essay_3']),\n",
    "                                                     str(row['project_essay_4'])]), axis=1)\n",
    "\n",
    "T = T.drop(['project_essay_3', 'project_essay_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Features\n",
    "Here we extract some features from the resource file. For each application, there are some resources listed in this file. We can extract how many items and at what prices are requested. minimum, maximum and average price and quantity of each item and for all requested items per application can be important in the decision-making process.\n",
    "\n",
    "Also, I combine the resource description columns and make a new text column in table T. Later, we will do text analysis on this column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "R['priceAll'] = R['quantity']*R['price']\n",
    "newR = R.groupby('id').agg({'description':'count',\n",
    "                            'quantity':'sum',\n",
    "                            'price':'sum',\n",
    "                            'priceAll':'sum'}).rename(columns={'description':'items'})\n",
    "newR['avgPrice'] = newR.priceAll / newR.quantity\n",
    "numFeatures = ['items', 'quantity', 'price', 'priceAll', 'avgPrice']\n",
    "\n",
    "for func in ['min', 'max', 'mean']:\n",
    "    newR = newR.join(R.groupby('id').agg({'quantity':func,\n",
    "                                          'price':func,\n",
    "                                          'priceAll':func}).rename(\n",
    "                                columns={'quantity':func+'Quantity',\n",
    "                                         'price':func+'Price',\n",
    "                                         'priceAll':func+'PriceAll'}).fillna(0))\n",
    "    numFeatures += [func+'Quantity', func+'Price', func+'PriceAll']\n",
    "\n",
    "newR = newR.join(R.groupby('id').agg(\n",
    "    {'description':lambda x:' '.join(x.values.astype(str))}).rename(\n",
    "    columns={'description':'resource_description'}))\n",
    "\n",
    "T = T.join(newR, on='id')\n",
    "\n",
    "# if you visit the donors website, it has categorized the price by these bins:\n",
    "T['price_category'] = pl.digitize(T.priceAll, [0, 50, 100, 250, 500, 1000, pl.inf])\n",
    "numFeatures.append('price_category')\n",
    "# the difference of max and min of price and quantity per item can also be relevant\n",
    "for c in ['Quantity', 'Price', 'PriceAll']:\n",
    "    T['max%s_min%s'%(c,c)] = T['max%s'%c] - T['min%s'%c]\n",
    "    numFeatures.append('max%s_min%s'%(c,c))\n",
    "\n",
    "del Ttr, Tts, R, newR\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Features\n",
    "We know some teachers have applied many times, and knowing the history of their applications, can be helpful to predict approval. So, I convert the teacher_id to numeric values and include it in my numeric features.\n",
    "\n",
    "Often times, knowing the statistics of categorical features, i.e. knowing how many times a certain value has repeated in the dataset can help. So let's extract this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "T['teacher_id'] = le.fit_transform(T['teacher_id'])\n",
    "T['teacher_gender_unknown'] = T.teacher_prefix.apply(lambda x:int(x not in ['Ms.', 'Mrs.', 'Mr.']))\n",
    "numFeatures += ['teacher_number_of_previously_posted_projects','teacher_id','teacher_gender_unknown']\n",
    "\n",
    "statFeatures = []\n",
    "for col in ['school_state', 'teacher_id', 'teacher_prefix', 'teacher_gender_unknown', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories', 'teacher_number_of_previously_posted_projects']:\n",
    "    Stat = T[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})\n",
    "    Stat /= Stat.sum()\n",
    "    T = T.join(Stat, on=col)\n",
    "    statFeatures.append(col+'_stat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis\n",
    "With the help of textblob module, we can find polarity and subjectivity of texts to some extent. It is, unfortunately, a little time-consuming. There might be other modules that work faster like VADER-Sentiment. Though, I haven't checked other modules. Their quality of analysis can also be different. Have you ever tried other modules? Do you know any better one?\n",
    "\n",
    "Another way of doing (sort of) sentimental analysis is to check for certain words and characters in the texts. I, personally, for example, feel uncomfortable if a text has so many exclamation marks :D. But, seriously, some of these may have an unconscious effect on the examiner. For example, if any words are bolded by \", or the number of sentences (number of \".\"), number of paragraphs (\\r), talking about money ($) or percentages (%), having a URL (http), etc. can influence the decision. What other words or characters do you think can be important?\n",
    "\n",
    "Talking about I or WE and having positive or negative words and phrases like that can also be influential. In one of the following sections (Text Features), by extracting n-grams, I hope to catch such phrases if they appear as repeated patterns.\n",
    "\n",
    "The number of words or the length of the texts can be another factor that can influence the decision unconsciously (or even consciously!). Number of transitional words, verbs, adjectives, adverbs, etc. in an essay can also indicate some aspects of the quality of the text.\n",
    "\n",
    "But, certainly, the quality of the essays is the most effective factor in my opinion. Things like the grammar errors, spelling errors, quality of the texts, word choices etc. are very important. Another important factor, if I was a grant examiner, would have been to check if the application writer could relate their needs to the resources they want through essays and project title. One primitive way to do this is to check for common words in different texts. Let me know if you know any better way to do these type of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimental analysis\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "textColumns = ['project_essay_1', 'project_essay_2', 'project_resource_summary', 'resource_description', 'project_title']\n",
    "\n",
    "def getSentFeat(s):\n",
    "    sent = TextBlob(s).sentiment\n",
    "    return (sent.polarity, sent.subjectivity)\n",
    "\n",
    "print('sentimental analysis')\n",
    "with Pool(4) as p:\n",
    "    for col in textColumns:\n",
    "        temp = pl.array(list(p.map(getSentFeat, T[col])))\n",
    "        T[col+'_pol'] = temp[:,0]\n",
    "        T[col+'_sub'] = temp[:,1]\n",
    "        numFeatures += [col+'_pol', col+'_sub']\n",
    "\n",
    "print('key words')\n",
    "KeyChars = ['!', '\\?', '@', '#', '\\$', '%', '&', '\\*', '\\(', '\\[', '\\{', '\\|', '-', '_', '=', '\\+',\n",
    "            '\\.', ':', ';', ',', '/', '\\\\\\\\r', '\\\\\\\\t', '\\\\\"', '\\.\\.\\.', 'etc', 'http', 'poor',\n",
    "            'military', 'traditional', 'charter', 'head start', 'magnet', 'year-round', 'alternative',\n",
    "            'art', 'book', 'basics', 'computer', 'laptop', 'tablet', 'kit', 'game', 'seat',\n",
    "            'food', 'cloth', 'hygiene', 'instraction', 'technolog', 'lab', 'equipment',\n",
    "            'music', 'instrument', 'nook', 'desk', 'storage', 'sport', 'exercise', 'trip', 'visitor',\n",
    "            'my students', 'our students', 'my class', 'our class']\n",
    "for col in textColumns:\n",
    "    for c in KeyChars:\n",
    "        T[col+'_'+c] = T[col].apply(lambda x: len(re.findall(c, x.lower())))\n",
    "        numFeatures.append(col+'_'+c)\n",
    "\n",
    "#####\n",
    "print('num words')\n",
    "for col in textColumns:\n",
    "    T['n_'+col] = T[col].apply(lambda x: len(x.split()))\n",
    "    numFeatures.append('n_'+col)\n",
    "    T['nUpper_'+col] = T[col].apply(lambda x: sum([s.isupper() for s in list(x)]))\n",
    "    numFeatures.append('nUpper_'+col)\n",
    "\n",
    "#####\n",
    "print('word tags')\n",
    "Tags = ['CC', 'CD', 'DT', 'IN', 'JJ', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', \n",
    "        'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', \n",
    "        'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "def getTagFeat(s):\n",
    "    d = Counter([t[1] for t in pos_tag(s.split())])\n",
    "    return [d[t] for t in Tags]\n",
    "\n",
    "with Pool(4) as p:\n",
    "    for col in textColumns:\n",
    "        temp = pl.array(list(p.map(getTagFeat, T[col])))\n",
    "        for i, t in enumerate(Tags):\n",
    "            if temp[:,i].sum() == 0:\n",
    "                continue\n",
    "            T[col+'_'+t] = temp[:, i]\n",
    "            numFeatures += [col+'_'+t]\n",
    "\n",
    "#####\n",
    "print('common words')\n",
    "for i, col1 in enumerate(textColumns[:-1]):\n",
    "    for col2 in textColumns[i+1:]:\n",
    "        T['%s_%s_common' % (col1, col2)] = T.apply(lambda row:len(set(re.split('\\W', row[col1].lower())).intersection(re.split('\\W', row[col2].lower()))), axis=1)\n",
    "        numFeatures.append('%s_%s_common' % (col1, col2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess what! someone didn't like !s in essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(15,5))\n",
    "sns.violinplot(data=T,x=target,y='project_essay_2_!');\n",
    "pl.figure(figsize=(15,5))\n",
    "sns.violinplot(data=T,x=target,y='project_essay_1_!');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Features\n",
    "The time at which the proposal was submitted can be important. Most importantly, we know thanks to Heads or Tails that there is a slight approval rate modulation over time. So we need to extract date info. Day of the week it has been posted can also play a role. I doubt if the hour it was submitted has any significance, but let's let the decision trees take care of that. Next, let's extract some statistics from time features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCol = 'project_submitted_datetime'\n",
    "def getTimeFeatures(T):\n",
    "    T['year'] = T[dateCol].apply(lambda x: x.year)\n",
    "    T['month'] = T[dateCol].apply(lambda x: x.month)\n",
    "    T['day'] = T[dateCol].apply(lambda x: x.day)\n",
    "    T['dow'] = T[dateCol].apply(lambda x: x.dayofweek)\n",
    "    T['hour'] = T[dateCol].apply(lambda x: x.hour)\n",
    "    T['days'] = (T[dateCol]-T[dateCol].min()).apply(lambda x: x.days)\n",
    "    return T\n",
    "\n",
    "T[dateCol] = pd.to_datetime(T[dateCol])\n",
    "T = getTimeFeatures(T)\n",
    "\n",
    "P_tar = T[T.tr==1][target].mean()\n",
    "timeFeatures = ['year', 'month', 'day', 'dow', 'hour', 'days']\n",
    "for col in timeFeatures:\n",
    "    Stat = T[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})\n",
    "    Stat /= Stat.sum()\n",
    "    T = T.join(Stat, on=col)\n",
    "    statFeatures.append(col+'_stat')\n",
    "\n",
    "numFeatures += timeFeatures\n",
    "numFeatures += statFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Features\n",
    "So far, I have extracted some numerical features. Often it helps the decision trees to provide some polynomial features to them. Here I include first-order interaction polynomials, and I check for the significance of the new variable before adding it to the columns. I add it only if it really helps to predict the approval better. A trick that I'm using here is that, maybe, the division of two variables is more significantly predicting the target! That would be the case if 1/V is a more significant predictor than V. So, I check for the significance of 1/(V+1) and V+1 (+1 is to avoid production or division by 0), and replace the most significant one to the original variable V. What do you think about this? It certainly helped though!\n",
    "\n",
    "By checking the significance and correlation in training set, there will be an over-training chance, which I'm trying to decrease by computing the average of correlations and p-values over randomly selected subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "T2 = T[numFeatures+['id','tr','ts',target]].copy()\n",
    "Ttr = T2[T.tr==1]\n",
    "Tar_tr = Ttr[target].values\n",
    "n = 10\n",
    "inx = [pl.randint(0, Ttr.shape[0], int(Ttr.shape[0]/n)) for k in range(n)]\n",
    "# inx is used for crossvalidation of calculating the correlation and p-value\n",
    "Corr = {}\n",
    "for c in numFeatures:\n",
    "    # since some values might be 0s, I use x+1 to avoid missing some important relations\n",
    "    C1,P1=pl.nanmean([pearsonr(Tar_tr[inx[k]],   (1+Ttr[c].iloc[inx[k]])) for k in range(n)], 0)\n",
    "    C2,P2=pl.nanmean([pearsonr(Tar_tr[inx[k]], 1/(1+Ttr[c].iloc[inx[k]])) for k in range(n)], 0)\n",
    "    if P2<P1:\n",
    "        T2[c] = 1/(1+T2[c])\n",
    "        Corr[c] = [C2,P2]\n",
    "    else:\n",
    "        T2[c] = 1+T2[c]\n",
    "        Corr[c] = [C1,P1]\n",
    "\n",
    "polyCol = []\n",
    "thrP = 0.01\n",
    "thrC = 0.02\n",
    "print('columns \\t\\t\\t Corr1 \\t\\t Corr2 \\t\\t Corr Combined')\n",
    "for i, c1 in enumerate(numFeatures[:-1]):\n",
    "    C1, P1 = Corr[c1]\n",
    "    for c2 in numFeatures[i+1:]:\n",
    "        C2, P2 = Corr[c2]\n",
    "        V = T2[c1] * T2[c2]\n",
    "        Vtr = V[T2.tr==1].values\n",
    "        C, P = pl.nanmean([pearsonr(Tar_tr[inx[k]], Vtr[inx[k]]) for k in range(n)], 0)\n",
    "        if P<thrP and abs(C) - max(abs(C1),abs(C2)) > thrC:\n",
    "            T[c1+'_'+c2+'_poly'] = V\n",
    "            polyCol.append(c1+'_'+c2+'_poly')\n",
    "            print(c1+'_'+c2, '\\t\\t(%g, %g)\\t(%g, %g)\\t(%g, %g)'%(C1,P1, C2,P2, C,P))\n",
    "\n",
    "numFeatures += polyCol\n",
    "print(len(numFeatures))\n",
    "del T2, Ttr\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the variable created out of maxPrice and meanPrice is much more informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(15,5));sns.violinplot(data=T,x=target,y='maxPrice')\n",
    "pl.figure(figsize=(15,5));sns.violinplot(data=T,x=target,y='meanPrice')\n",
    "pl.figure(figsize=(15,5));sns.violinplot(data=T,x=target,y='maxPrice_meanPrice_poly');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features\n",
    "Next, we include categorical features. Categorical features are teacher prefix, state, grade, and subject categories. We previously were doing it wrong by taking onehot or label encoder, in particular for project subject categorirs and sub-categories. For example, \"Performing Arts, Team Sports\" is in fact a combination of two categories: \"Performing Arts\" and \"Team Sports\" which previously we were taking this combination as a sole category. I've fixed it here by using count vectorizers. Now we have 9 categories and 30 sub-categories. It should be actually 8 categories since \"Warmth, Care & Hunger\" is one category which our algorithm takes it as two categories when it splits by comma, but it doesn't affect our results dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCatFeatures(T, Col):\n",
    "    vectorizer = CountVectorizer(binary=True,\n",
    "                                 ngram_range=(1,1),\n",
    "                                 tokenizer=lambda x:[a.strip() for a in x.split(',')])\n",
    "    return vectorizer.fit_transform(T[Col].fillna(''))\n",
    "\n",
    "X_tp = getCatFeatures(T, 'teacher_prefix')\n",
    "X_ss = getCatFeatures(T, 'school_state')\n",
    "X_pgc = getCatFeatures(T, 'project_grade_category')\n",
    "X_psc = getCatFeatures(T, 'project_subject_categories')\n",
    "X_pssc = getCatFeatures(T, 'project_subject_subcategories')\n",
    "\n",
    "X_cat = hstack((X_tp, X_ss, X_pgc, X_psc, X_pssc))\n",
    "\n",
    "del X_tp, X_ss, X_pgc, X_psc, X_pssc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Features\n",
    "Finally, we do text analysis. For this section, I used both Tf-IDF and count vectorizer and interestingly, count vectorizer with binary features, showing only if a word is in the text, has the best performance in my experience. Other than that, since there are mis-spellings in the texts, it would have helped to check for spelling errors first. I found \"TextBlob\" and \"autocorrect\" modules for this purpose but, unfortunately, it was so slow and I didn't use it at last. Do you know any better way to do that?Also, I decided not using any stop words because some of them can actually be useful in this case and after all they are only a few words.\n",
    "\n",
    "I tried using dimensionality reduction techniques to reduce the dimensions following the idea of Latent Semantic Analysis, but it didn't help the prediction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from autocorrect import spell  # as spell checker and corrector\n",
    "# L = WordNetLemmatizer()\n",
    "p = PorterStemmer()\n",
    "def wordPreProcess(sentence):\n",
    "    return ' '.join([p.stem(x.lower()) for x in re.split('\\W', sentence) if len(x) >= 1])\n",
    "# return ' '.join([p.stem(L.lemmatize(spell(x.lower()))) for x in re.split('\\W', sentence) if len(x) > 1])\n",
    "\n",
    "\n",
    "def getTextFeatures(T, Col, max_features=10000, ngrams=(1,2), verbose=True):\n",
    "    if verbose:\n",
    "        print('processing: ', Col)\n",
    "    vectorizer = CountVectorizer(stop_words=None,\n",
    "                                 preprocessor=wordPreProcess,\n",
    "                                 max_features=max_features,\n",
    "                                 binary=True,\n",
    "                                 ngram_range=ngrams)\n",
    "#     vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "#                                  preprocessor=wordPreProcess,\n",
    "#                                  max_features=max_features)\n",
    "    X = vectorizer.fit_transform(T[Col])\n",
    "    return X, vectorizer.get_feature_names()\n",
    "\n",
    "n_es1, n_es2, n_prs, n_rd, n_pt = 3000, 8000, 2000, 3000, 1000\n",
    "X_es1, feat_es1 = getTextFeatures(T, 'project_essay_1', max_features=n_es1)\n",
    "X_es2, feat_es2 = getTextFeatures(T, 'project_essay_2', max_features=n_es2)\n",
    "X_prs, feat_prs = getTextFeatures(T, 'project_resource_summary', max_features=n_prs)\n",
    "X_rd, feat_rd = getTextFeatures(T, 'resource_description', max_features=n_rd, ngrams=(1,3))\n",
    "X_pt, feat_pt = getTextFeatures(T, 'project_title', max_features=n_pt)\n",
    "\n",
    "X_txt = hstack((X_es1, X_es2, X_prs, X_rd, X_pt))\n",
    "del X_es1, X_es2, X_prs, X_rd, X_pt\n",
    "\n",
    "# \n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# svd = TruncatedSVD(1000)\n",
    "# X_txt = svd.fit_transform(X_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make up the train and test matrices:\n",
    "\n",
    "we should normalize the values if we want to use neural networks. Since my sparse features are 0s and 1s, I only apply it to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = hstack((X_txt, X_cat, StandardScaler().fit_transform(T[numFeatures].fillna(0)))).tocsr()\n",
    "\n",
    "Xtr = X[pl.find(T.tr==1), :]\n",
    "Xts = X[pl.find(T.ts==1), :]\n",
    "Ttr_tar = T[T.tr==1][target].values\n",
    "Tts = T[T.ts==1][['id',target]]\n",
    "\n",
    "Yts = []\n",
    "del T\n",
    "del X\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, concatenate, Dropout, Embedding, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def breakInput(X1):\n",
    "    X2 = []\n",
    "    i = 0\n",
    "    for n in [n_es1, n_es2, n_prs, n_rd, n_pt, X_cat.shape[1], len(numFeatures)]:\n",
    "        X2.append(X1[:,i:i+n])\n",
    "        i += n\n",
    "    return X2\n",
    "\n",
    "def getModel(HLs, Drop=0.25, OP=optimizers.Adam()):\n",
    "    temp = []\n",
    "    inputs_txt = []\n",
    "    for n in [n_es1, n_es2, n_prs, n_rd, n_pt]:\n",
    "        input_txt = Input((n, ))\n",
    "        X_feat = Dropout(Drop)(input_txt)\n",
    "        X_feat = Dense(int(n/100), activation=\"linear\")(X_feat)\n",
    "        X_feat = Dropout(Drop)(X_feat)\n",
    "        temp.append(X_feat)\n",
    "        inputs_txt.append(input_txt)\n",
    "\n",
    "    x_1 = concatenate(temp)\n",
    "#     x_1 = Dense(20, activation=\"relu\")(x_1)\n",
    "    x_1 = Dense(50, activation=\"relu\")(x_1)\n",
    "    x_1 = Dropout(Drop)(x_1)\n",
    "\n",
    "    input_cat = Input((X_cat.shape[1], ))\n",
    "    x_2 = Embedding(2, 10, input_length=X_cat.shape[1])(input_cat)\n",
    "    x_2 = SpatialDropout1D(Drop)(x_2)\n",
    "    x_2 = Flatten()(x_2)\n",
    "\n",
    "    input_num = Input((len(numFeatures), ))\n",
    "    x_3 = Dropout(Drop)(input_num)\n",
    "    \n",
    "    x = concatenate([x_1, x_2, x_3])\n",
    "\n",
    "    for HL in HLs:\n",
    "        x = Dense(HL, activation=\"relu\")(x)\n",
    "        x = Dropout(Drop)(x)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs_txt+[input_cat, input_num], outputs=output)\n",
    "    model.compile(\n",
    "            optimizer=OP,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "def trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam()):\n",
    "    file_path='NN.h5'\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6)\n",
    "    lr_reduced = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.5,\n",
    "                                   patience=2,\n",
    "                                   verbose=1,\n",
    "                                   epsilon=3e-4,\n",
    "                                   mode='min')\n",
    "\n",
    "    model = getModel(HL, Drop, OP)\n",
    "    model.fit(breakInput(X_train), Tar_train, validation_data=(breakInput(X_val), Tar_val),\n",
    "                        verbose=2, epochs=50, batch_size=1000, callbacks=[early, lr_reduced, checkpoint])\n",
    "    model.load_weights(file_path)\n",
    "    return model\n",
    "\n",
    "params_xgb = {\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 4,\n",
    "        'subsample': 0.85,\n",
    "        'colsample_bytree': 0.25,\n",
    "        'min_child_weight': 3,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 0,\n",
    "        'silent': 1,\n",
    "    }\n",
    "params_lgb = {\n",
    "        'boosting_type': 'dart',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.25,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'seed': 0,\n",
    "        'verbose': 0,\n",
    "    }\n",
    "nCV = 1 # should be ideally larger\n",
    "for i in range(21, 22):\n",
    "    gc.collect()\n",
    "    X_train, X_val, Tar_train, Tar_val = train_test_split(Xtr, Ttr_tar, test_size=0.15, random_state=i, stratify=Ttr_tar)\n",
    "    # XGB\n",
    "    dtrain = xgb.DMatrix(X_train, label=Tar_train)\n",
    "    dval   = xgb.DMatrix(X_val, label=Tar_val)\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "    model = xgb.train(params_xgb, dtrain, 5000,  watchlist, maximize=True, verbose_eval=200, early_stopping_rounds=200)\n",
    "    Yvl1 = model.predict(dval)\n",
    "    Yts1 = model.predict(xgb.DMatrix(Xts))\n",
    "    # LGB\n",
    "    dtrain = lgb.Dataset(X_train, Tar_train)\n",
    "    dval   = lgb.Dataset(X_val, Tar_val)\n",
    "    model = lgb.train(params_lgb, dtrain, num_boost_round=10000, valid_sets=[dtrain, dval], early_stopping_rounds=200, verbose_eval=200)\n",
    "    Yvl2 = model.predict(X_val)\n",
    "    Yts2 = model.predict(Xts)\n",
    "    # NN\n",
    "    model = trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam())\n",
    "    Yvl3 = model.predict(breakInput(X_val)).squeeze()\n",
    "    Yts3 = model.predict(breakInput(Xts)).squeeze()\n",
    "    # stack\n",
    "    M = LinearRegression()\n",
    "    M.fit(pl.array([Yvl1, Yvl2, Yvl3]).T, Tar_val)\n",
    "    Yts.append(M.predict(pl.array([Yts1, Yts2, Yts3]).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output for Test Set\n",
    "At last, we make the stack of test set outputs by simple averaging, maybe rank average or median work better, I didn't try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Tts[target] = MinMaxScaler().fit_transform(pl.array(Yts).mean(0).reshape(-1,1))\n",
    "Tts[['id', target]].to_csv('text_cat_num_xgb_lgb_NN.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
