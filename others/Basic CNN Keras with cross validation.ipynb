{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mimg\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# DEEP LEARNING IMPORTS\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Activation, Dropout, Flatten, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding function\n",
    "def one_hot_encoder(df_name, df_column_name):\n",
    "    temp = pd.get_dummies(df_name[df_column_name]) #get dummies is used to create dummy columns\n",
    "    df_name = pd.concat([df_name,temp],axis=1) #join the newly created dummy columns to original dataframe\n",
    "    df_name = df_name.drop(df_column_name, axis=1) #drop the old column used to create dummy columnss\n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to draw confusion matrix\n",
    "def draw_confusion_matrix(true,preds):\n",
    "    conf_matx = confusion_matrix(true, preds)\n",
    "    sns.heatmap(conf_matx, annot=True,annot_kws={\"size\": 12},fmt='g', cbar=False, cmap=\"viridis\")\n",
    "    plt.show()\n",
    "    #return conf_matx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pd.read_csv(\"fashion-mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_x = train_images.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_array = train_images_x.values\n",
    "train_x = train_images_array.reshape(train_images_array.shape[0], 28, 28, 1)\n",
    "train_x_scaled = train_x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training labels and one hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8  9\n",
      "0  0  0  1  0  0  0  0  0  0  0\n",
      "1  0  0  0  0  0  0  0  0  0  1\n",
      "2  0  0  0  0  0  0  1  0  0  0\n",
      "3  1  0  0  0  0  0  0  0  0  0\n",
      "4  0  0  0  1  0  0  0  0  0  0\n"
     ]
    }
   ],
   "source": [
    "### read the image labels and one hot encode the labels\n",
    "train_images_y = train_images[['label']]\n",
    "\n",
    "#do one hot encoding with the earlier created function\n",
    "train_images_y_encoded = one_hot_encoder(train_images_y, 'label')\n",
    "print(train_images_y_encoded.head())\n",
    "\n",
    "#get the labels as an array\n",
    "train_images_y_encoded = train_images_y_encoded.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d7874e92e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7874e9390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check to see if distribution of target labels are equal (if not equal we need to assign weights to classes)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(train_images_y['label'].value_counts().index, \n",
    "        train_images_y['label'].value_counts().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = pd.read_csv(\"fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_x = test_images.iloc[:,1:]\n",
    "\n",
    "test_images_array = test_images_x.values\n",
    "test_x = test_images_array.reshape(test_images_array.shape[0], 28, 28, 1)\n",
    "test_x_scaled = test_x/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8  9\n",
      "0  1  0  0  0  0  0  0  0  0  0\n",
      "1  0  1  0  0  0  0  0  0  0  0\n",
      "2  0  0  1  0  0  0  0  0  0  0\n",
      "3  0  0  1  0  0  0  0  0  0  0\n",
      "4  0  0  0  1  0  0  0  0  0  0\n"
     ]
    }
   ],
   "source": [
    "test_images_y = test_images[['label']]\n",
    "test_images_y_encoded = one_hot_encoder(test_images_y, 'label')\n",
    "print(test_images_y_encoded.head())\n",
    "#get the labels as an array\n",
    "test_images_y_encoded = test_images_y_encoded.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(train_x_scaled, \n",
    "        train_images_y_encoded, random_state = 101,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(size, num_cnn_layers):\n",
    "    NUM_FILTERS = 32\n",
    "    KERNEL = (3, 3)\n",
    "    #MIN_NEURONS = 20\n",
    "    MAX_NEURONS = 120\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(1, num_cnn_layers+1):\n",
    "        if i == 1:\n",
    "            model.add(Conv2D(NUM_FILTERS*i, KERNEL, input_shape=size, activation='relu', padding='same'))\n",
    "        else:\n",
    "            model.add(Conv2D(NUM_FILTERS*i, KERNEL, activation='relu', padding='same'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(int(MAX_NEURONS), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(int(MAX_NEURONS/2), activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model(IMAGE_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               1505400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 1,532,086\n",
      "Trainable params: 1,532,086\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set early stopping criteria\n",
    "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "\n",
    "#define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
    "model_checkpoint = ModelCheckpoint('fas_mnist_1.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "#define a function to fit the model\n",
    "def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=20, BATCH_SIZE=128):\n",
    "    model = None\n",
    "    model = cnn_model(IMAGE_SIZE, 2)\n",
    "    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], \n",
    "              verbose=1, validation_split=0.1)  \n",
    "    print(\"Val Score: \", model.evaluate(val_x, val_y))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold:  1\n",
      "Train on 36450 samples, validate on 4050 samples\n",
      "Epoch 1/20\n",
      "36450/36450 [==============================] - 394s 11ms/step - loss: 0.5595 - acc: 0.8027 - val_loss: 0.3453 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34534, saving model to fas_mnist_1.h5\n",
      "Epoch 2/20\n",
      "36450/36450 [==============================] - 192s 5ms/step - loss: 0.3353 - acc: 0.8788 - val_loss: 0.2878 - val_acc: 0.8943\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34534 to 0.28777, saving model to fas_mnist_1.h5\n",
      "Epoch 3/20\n",
      "36450/36450 [==============================] - 99s 3ms/step - loss: 0.2723 - acc: 0.9020 - val_loss: 0.2514 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28777 to 0.25143, saving model to fas_mnist_1.h5\n",
      "Epoch 4/20\n",
      "36450/36450 [==============================] - 101s 3ms/step - loss: 0.2377 - acc: 0.9122 - val_loss: 0.2521 - val_acc: 0.9069\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25143\n",
      "Epoch 5/20\n",
      "36450/36450 [==============================] - 99s 3ms/step - loss: 0.2084 - acc: 0.9227 - val_loss: 0.2424 - val_acc: 0.9106\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25143 to 0.24238, saving model to fas_mnist_1.h5\n",
      "Epoch 6/20\n",
      "36450/36450 [==============================] - 101s 3ms/step - loss: 0.1797 - acc: 0.9341 - val_loss: 0.2408 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24238 to 0.24084, saving model to fas_mnist_1.h5\n",
      "Epoch 7/20\n",
      "36450/36450 [==============================] - 100s 3ms/step - loss: 0.1604 - acc: 0.9408 - val_loss: 0.2398 - val_acc: 0.9121\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24084 to 0.23985, saving model to fas_mnist_1.h5\n",
      "Epoch 8/20\n",
      "36450/36450 [==============================] - 100s 3ms/step - loss: 0.1396 - acc: 0.9485 - val_loss: 0.2400 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.23985\n",
      "Epoch 9/20\n",
      "36450/36450 [==============================] - 100s 3ms/step - loss: 0.1201 - acc: 0.9570 - val_loss: 0.2774 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.23985\n",
      "Epoch 10/20\n",
      "36450/36450 [==============================] - 121s 3ms/step - loss: 0.1006 - acc: 0.9639 - val_loss: 0.2772 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.23985\n",
      "Epoch 11/20\n",
      "36450/36450 [==============================] - 95s 3ms/step - loss: 0.0867 - acc: 0.9685 - val_loss: 0.2889 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.23985\n",
      "Epoch 12/20\n",
      "36450/36450 [==============================] - 98s 3ms/step - loss: 0.0776 - acc: 0.9711 - val_loss: 0.2960 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.23985\n",
      "Epoch 00012: early stopping\n",
      "4500/4500 [==============================] - 4s 863us/step\n",
      "Val Score:  [0.2951511232058207, 0.9186666667726304]\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold:  2\n",
      "Train on 36450 samples, validate on 4050 samples\n",
      "Epoch 1/20\n",
      "36450/36450 [==============================] - 99s 3ms/step - loss: 0.5864 - acc: 0.7901 - val_loss: 0.3510 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.23985\n",
      "Epoch 2/20\n",
      "36450/36450 [==============================] - 97s 3ms/step - loss: 0.3518 - acc: 0.8740 - val_loss: 0.3064 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.23985\n",
      "Epoch 3/20\n",
      "36450/36450 [==============================] - 98s 3ms/step - loss: 0.2880 - acc: 0.8960 - val_loss: 0.2689 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.23985\n",
      "Epoch 4/20\n",
      "36450/36450 [==============================] - 98s 3ms/step - loss: 0.2528 - acc: 0.9075 - val_loss: 0.2458 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.23985\n",
      "Epoch 5/20\n",
      "36450/36450 [==============================] - 97s 3ms/step - loss: 0.2232 - acc: 0.9166 - val_loss: 0.2342 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23985 to 0.23418, saving model to fas_mnist_1.h5\n",
      "Epoch 6/20\n",
      "36450/36450 [==============================] - 99s 3ms/step - loss: 0.1930 - acc: 0.9292 - val_loss: 0.2246 - val_acc: 0.9099\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23418 to 0.22456, saving model to fas_mnist_1.h5\n",
      "Epoch 7/20\n",
      "36450/36450 [==============================] - 96s 3ms/step - loss: 0.1717 - acc: 0.9365 - val_loss: 0.2225 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22456 to 0.22248, saving model to fas_mnist_1.h5\n",
      "Epoch 8/20\n",
      "36450/36450 [==============================] - 97s 3ms/step - loss: 0.1508 - acc: 0.9445 - val_loss: 0.2375 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22248\n",
      "Epoch 9/20\n",
      "36450/36450 [==============================] - 95s 3ms/step - loss: 0.1357 - acc: 0.9493 - val_loss: 0.2303 - val_acc: 0.9227\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22248\n",
      "Epoch 10/20\n",
      "36450/36450 [==============================] - 95s 3ms/step - loss: 0.1186 - acc: 0.9558 - val_loss: 0.2370 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22248\n",
      "Epoch 11/20\n",
      "36450/36450 [==============================] - 96s 3ms/step - loss: 0.1069 - acc: 0.9606 - val_loss: 0.2417 - val_acc: 0.9212\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22248\n",
      "Epoch 12/20\n",
      "36450/36450 [==============================] - 97s 3ms/step - loss: 0.0911 - acc: 0.9664 - val_loss: 0.2498 - val_acc: 0.9215\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22248\n",
      "Epoch 00012: early stopping\n",
      "4500/4500 [==============================] - 4s 903us/step\n",
      "Val Score:  [0.26638806687792144, 0.9259999999470181]\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold:  3\n",
      "Train on 36450 samples, validate on 4050 samples\n",
      "Epoch 1/20\n",
      "36450/36450 [==============================] - 181s 5ms/step - loss: 0.5722 - acc: 0.7959 - val_loss: 0.3413 - val_acc: 0.8783\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.22248\n",
      "Epoch 2/20\n",
      "36450/36450 [==============================] - 181s 5ms/step - loss: 0.3418 - acc: 0.8777 - val_loss: 0.2913 - val_acc: 0.8941\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.22248\n",
      "Epoch 3/20\n",
      "36450/36450 [==============================] - 168s 5ms/step - loss: 0.2850 - acc: 0.8980 - val_loss: 0.2680 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.22248\n",
      "Epoch 4/20\n",
      "36450/36450 [==============================] - 178s 5ms/step - loss: 0.2453 - acc: 0.9119 - val_loss: 0.2715 - val_acc: 0.9022\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.22248\n",
      "Epoch 5/20\n",
      "36450/36450 [==============================] - 152s 4ms/step - loss: 0.2173 - acc: 0.9189 - val_loss: 0.2607 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.22248\n",
      "Epoch 6/20\n",
      "36450/36450 [==============================] - 96s 3ms/step - loss: 0.1886 - acc: 0.9304 - val_loss: 0.2447 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.22248\n",
      "Epoch 7/20\n",
      "36450/36450 [==============================] - 115s 3ms/step - loss: 0.1696 - acc: 0.9391 - val_loss: 0.2583 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22248\n",
      "Epoch 8/20\n",
      "36450/36450 [==============================] - 134s 4ms/step - loss: 0.1456 - acc: 0.9467 - val_loss: 0.2525 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22248\n",
      "Epoch 9/20\n",
      "36450/36450 [==============================] - 138s 4ms/step - loss: 0.1306 - acc: 0.9515 - val_loss: 0.2495 - val_acc: 0.9210\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22248\n",
      "Epoch 10/20\n",
      "36450/36450 [==============================] - 104s 3ms/step - loss: 0.1112 - acc: 0.9589 - val_loss: 0.2679 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22248\n",
      "Epoch 11/20\n",
      "36450/36450 [==============================] - 94s 3ms/step - loss: 0.0952 - acc: 0.9652 - val_loss: 0.2645 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22248\n",
      "Epoch 00011: early stopping\n",
      "4500/4500 [==============================] - 4s 829us/step\n",
      "Val Score:  [0.2434536729454994, 0.9257777777247959]\n",
      "====================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds=3\n",
    "epochs=20\n",
    "batch_size=128\n",
    "\n",
    "#save the model history in a list after fitting so that we can plot later\n",
    "model_history = [] \n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \",i+1)\n",
    "    t_x, val_x, t_y, val_y = train_test_split(train_x, train_y, test_size=0.1, \n",
    "                                               random_state = np.random.randint(1,1000, 1)[0])\n",
    "    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
