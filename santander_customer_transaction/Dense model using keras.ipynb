{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "target = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "X = train.iloc[:,2:].values\n",
    "y = train.iloc[:,1].values\n",
    "test = test.iloc[:,1:].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,\n",
    "                                                   random_state = 3017)\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples, validate on 30000 samples\n",
      "Epoch 1/18\n",
      " - 5s - loss: 0.4879 - acc: 0.7924 - val_loss: 0.3696 - val_acc: 0.8916\n",
      "Epoch 2/18\n",
      " - 4s - loss: 0.3459 - acc: 0.8916 - val_loss: 0.3098 - val_acc: 0.9004\n",
      "Epoch 3/18\n",
      " - 4s - loss: 0.3089 - acc: 0.8978 - val_loss: 0.2873 - val_acc: 0.9019\n",
      "Epoch 4/18\n",
      " - 4s - loss: 0.2897 - acc: 0.9003 - val_loss: 0.2735 - val_acc: 0.9036\n",
      "Epoch 5/18\n",
      " - 5s - loss: 0.2773 - acc: 0.9018 - val_loss: 0.2640 - val_acc: 0.9050\n",
      "Epoch 6/18\n",
      " - 5s - loss: 0.2685 - acc: 0.9037 - val_loss: 0.2576 - val_acc: 0.9068\n",
      "Epoch 7/18\n",
      " - 5s - loss: 0.2608 - acc: 0.9052 - val_loss: 0.2534 - val_acc: 0.9077\n",
      "Epoch 8/18\n",
      " - 6s - loss: 0.2561 - acc: 0.9060 - val_loss: 0.2505 - val_acc: 0.9089\n",
      "Epoch 9/18\n",
      " - 6s - loss: 0.2526 - acc: 0.9073 - val_loss: 0.2484 - val_acc: 0.9090\n",
      "Epoch 10/18\n",
      " - 6s - loss: 0.2488 - acc: 0.9081 - val_loss: 0.2468 - val_acc: 0.9099\n",
      "Epoch 11/18\n",
      " - 6s - loss: 0.2452 - acc: 0.9093 - val_loss: 0.2456 - val_acc: 0.9106\n",
      "Epoch 12/18\n",
      " - 5s - loss: 0.2448 - acc: 0.9096 - val_loss: 0.2446 - val_acc: 0.9109\n",
      "Epoch 13/18\n",
      " - 6s - loss: 0.2410 - acc: 0.9104 - val_loss: 0.2437 - val_acc: 0.9113\n",
      "Epoch 14/18\n",
      " - 6s - loss: 0.2391 - acc: 0.9108 - val_loss: 0.2429 - val_acc: 0.9114\n",
      "Epoch 15/18\n",
      " - 6s - loss: 0.2365 - acc: 0.9116 - val_loss: 0.2423 - val_acc: 0.9118\n",
      "Epoch 16/18\n",
      " - 5s - loss: 0.2356 - acc: 0.9117 - val_loss: 0.2417 - val_acc: 0.9123\n",
      "Epoch 17/18\n",
      " - 6s - loss: 0.2350 - acc: 0.9119 - val_loss: 0.2413 - val_acc: 0.9124\n",
      "Epoch 18/18\n",
      " - 6s - loss: 0.2322 - acc: 0.9131 - val_loss: 0.2410 - val_acc: 0.9126\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(400, input_dim=200))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(400))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "optimizer = Nadam(1e-5)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "es = EarlyStopping(patience=10, verbose=1)\n",
    "history = model.fit(X_train_scaled, y_train, batch_size=256, epochs=18, validation_split=0.2, verbose=2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
