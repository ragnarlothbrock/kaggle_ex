{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Hyperparameter Tuning using Grid and Random Search\n",
    "\n",
    "In this notebook, we will explore two methods for hyperparameter tuning a machine learning model. In contrast to model parameters which are learned during training, model hyperparameters are set by the data scientist ahead of training and control implementation aspects of the model. The weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist. Hyperparameters can be thought of as model settings. These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will not be the best across all datasets. The process of hyperparameter tuning (also called hyperparameter optimization) means finding the combination of hyperparameter values for a machine learning model that performs the best - as measured on a validation dataset - for a problem.\n",
    "\n",
    "(Quick Note: a lot of data scientists use the terms parameters and hyperparameters interchangeably to refer to the model settings. While this is technically incorrect, it's pretty common practice and it's usually possible to tell when they are referring to parameters learned during training versus hyperparameters. I'll try to stick to using model hyperparameters or model settings and I'll point out when I'm talking about a parameter that is learned during training. If you're still confused, this article may help you out!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several approaches to hyperparameter tuning\n",
    "\n",
    "Manual: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.\n",
    "\n",
    "Grid Search: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!\n",
    "\n",
    "Random search: set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources.\n",
    "\n",
    "Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement approaches 2 and 3 for a Gradient Boosting Machine Learning Model. In a future notebook, we will implement automated hyperparameter tuning using Bayesian optimization, specifically the Hyperopt library. If you want to get an idea of how automated hyperparameter tuning is done, check out this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.0 Model: Gradient Boosting Machine\n",
    "The Gradient Boosting Machine (GBM) has recently emerged as one of the top machine learning models. The GBM is extremely effective on structured data - where the information is in rows and columns - and medium sized datasets - where there are at most a few million observations. We will focus on this model because it is currently the top performing method for most competitions on Kaggle and because the performance is highly dependent on the hyperparameter choices. The basics you need to know about the GBM are that it is an ensemble method that works by training many individual learners, almost always decision trees. However, unlike in a random forest where the trees are trained in parallel, in a GBM, the trees are trained sequentially with each tree learning from the mistakes of the previous ones. The hundreds or thousands of weak learners are combined to make a single strong ensemble learner with the contributions of each individual learned during training using Gradient Descent (the weights of the individual trees would therefore be a model parameter).\n",
    "\n",
    "The GBM has many hyperparameters to tune that control both the overall ensemble (such as the learning rate) and the individual decision trees (such as the number of leaves in the tree or the maximum depth of the tree). It is difficult to know which combination of hyperparameters will work best based only on theory because there are complex interactions between hyperparameters. Hence the need for hyperparameter tuning: the only way to find the optimal hyperparameter values is to try many different combinations on a dataset!\n",
    "\n",
    "We will use the implementation of the Gradient Boosting Machine in the LightGBM library. This is a much faster (and some say more accurate) implementation than that available in Scikit-Learn.\n",
    "\n",
    "For more details of the Gradient Boosting Machine (GBM), check out this high-level blog post, or this in depth technical article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started\n",
    "With the necessary background out of the way, let's get started. For this notebook, we will work with a subset of the data consisting of 10000 rows. Hyperparameter tuning is extremely computationally expensive and working with the full dataset in a Kaggle Kernel would not be feasible for more than a few search iterations. However, the same ideas that we will implement here can be applied to the full dataset and while this notebook is specifically aimed at the GBM, the methods can be applied for any machine learning model.\n",
    "\n",
    "To \"test\" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. When we do hyperparameter tuning, it's crucial to not tune the hyperparameters on the testing data. We can only use the testing data a single time when we evaluate the final model that has been tuned on the validation data. To actually test our methods from this notebook, we would need to train the best model on all of the training data, make predictions on the actual testing data, and then submit our answers to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_FOLDS = 5\n",
    "MAX_EVALS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we read in the data and separate into a training set of 10000 observations and a \"testing set\" of 6000 observations. After creating the testing set, we cannot do any hyperparameter tuning with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('application_train.csv')\n",
    "\n",
    "# Sample 16000 rows (10000 for training, 6000 for testing)\n",
    "features = features.sample(n = 16000, random_state = 42)\n",
    "\n",
    "# Only numeric features\n",
    "features = features.select_dtypes('number')\n",
    "\n",
    "# Extract the labels\n",
    "labels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\n",
    "features = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
    "\n",
    "# Split into training and testing data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use only the numeric features to reduce the number of dimensions which will help speed up the hyperparameter search. Again, this is something we would not want to do on a real problem, but for demonstration purposes, it will allow us to see the concepts in practice (rather than waiting days/months for the search to finish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:  (10000, 104)\n",
      "Testing features shape:  (6000, 104)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training features shape: \", train_features.shape)\n",
    "print(\"Testing features shape: \", test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.1 Cross Validation\n",
    "To evaluate each combination of hyperparameter values, we need to score them on a validation set. The hyperparameters can not be tuned on the testing data. We can only use the testing data once when we evaluate the final model. The testing data is meant to serve as an estimate of the model performance when deployed on real data, and therefore we do not want to optimize our model to the testing data because that will not give us a fair estimate of the actual performance. The correct approach is therefore to use a validation set. However, instead of splitting the valuable training data into a separate training and validation set, we use KFold cross validation. In addition to preserving training data, this should give us a better estimate of generalization performance on the test set than using a single validation set (since then we are probably overfitting to that validation set). The performance of each set of hyperparameters is determined by Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.\n",
    "\n",
    "In this example, we will use 5-fold cross validation which means training and testing the model with each set of hyperparameter values 5 times to assess performance. Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have a large enough training set, we can probably get away with just using a single separate validation set, but cross validation is a safer method to avoid overfitting.\n",
    "\n",
    "To implement KFold cross validation, we will use the LightGBM cross validation function, cv, because this allows us to use a critical technique for training a GBM, early stopping. (For other machine learning models where we do not need to use early stopping, we can use the Scikit-Learn functions RandomizedSearchCV or GridSearchCV.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.2 Early Stopping\n",
    "One of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators (the number of decision trees trained sequentially). We could set this as another hyperparameter in our search, but there's a better method: early stopping. Early stopping means training until the validation error does not decrease for a specified number of iterations. In the case of the GBM, this means training more decision trees, and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation error has not decreased for 100 rounds. Then, the number of estimators that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model.\n",
    "\n",
    "The concept of early stopping is commonly applied to the GBM and to deep neural networks so it's a great technique to understand. This is one of many forms of regularization that aims to improve generalization performance on the testing set by not overfitting to the training data. If we keep adding estimators, the training error will always decrease because the capacity of the model increases. Although this might seem positive, it means that the model will start to memorize the training data and then will not perform well on new testing data. The variance of the model increases as we continue adding estimators because the model starts to rely too heavily on the training data (high variance means overfitting).\n",
    "\n",
    "Early stopping is simple to implement with the LightGBM library in the cross validation function. We simply need to pass in the number of early stopping rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.3 Example of Cross Validation and Early Stopping\n",
    "To use the cv function, we first need to make a LightGBM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and testing dataset\n",
    "train_set = lgb.Dataset(data = train_features, label = train_labels)\n",
    "test_set = lgb.Dataset(data = test_features, label = test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to pass in a set of hyperparameters to the cross validation, so we will use the default hyperparameters in LightGBM. In the cv call, the num_boost_round is set to 10,000 (num_boost_round is the same as n_estimators), but this number won't actually be reached because we are using early stopping. As a reminder, the metric we are using is Receiver Operating Characteristic Area Under the Curve (ROC AUC).\n",
    "\n",
    "The code below carries out both cross validation with 5 folds and early stopping with 100 early stopping rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:657: UserWarning: silent keyword has been found in `params` and will be ignored. Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n"
     ]
    }
   ],
   "source": [
    "# Get default hyperparameters\n",
    "model = lgb.LGBMClassifier()\n",
    "default_params = model.get_params()\n",
    "\n",
    "# Remove the number of estimators because we set this to 10000 in the cv call\n",
    "del default_params['n_estimators']\n",
    "\n",
    "# Cross validation with early stopping\n",
    "cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = N_FOLDS, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum validation ROC AUC was: 0.70147 with a standard deviation of 0.01872.\n",
      "The optimal number of boosting rounds (estimators) was 35.\n"
     ]
    }
   ],
   "source": [
    "print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n",
    "print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this result as a baseline model to beat. To find out how well the model does on our \"test\" data, we will retrain it on all the training data with the best number of estimators found during cross validation with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model scores 0.72566 ROC AUC on the test set.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Optimal number of esimators found in cv\n",
    "model.n_estimators = len(cv_results['auc-mean'])\n",
    "\n",
    "# Train and make predicions with model\n",
    "model.fit(train_features, train_labels)\n",
    "preds = model.predict_proba(test_features)[:, 1]\n",
    "baseline_auc = roc_auc_score(test_labels, preds)\n",
    "\n",
    "print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04733578, 0.11786518, 0.09660675, ..., 0.14284931, 0.03865695,\n",
       "       0.10858285])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.4 Hyperparameter Tuning Implementation\n",
    "Now we have the basic framework in place: we will use cross validation to determine the performance of model hyperparameters and early stopping with the GBM so we do not have to tune the number of estimators. The basic strategy for both grid and random search is simple: for each hyperparameter value combination, evaluate the cross validation score and record the results along with the hyperparameters. Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score, train the model on all the training data, and make predictions on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four parts of Hyperparameter tuning\n",
    "It's helpful to think of hyperparameter tuning as having four parts (these four parts also will form the basis of Bayesian Optimization):\n",
    "\n",
    "Objective function: a function that takes in hyperparameters and returns a score we are trying to minimize or maximize\n",
    "Domain: the set of hyperparameter values over which we want to search.\n",
    "Algorithm: method for selecting the next set of hyperparameters to evaluate in the objective function.\n",
    "Results history: data structure containing each set of hyperparameters and the resulting score from the objective function.\n",
    "Switching from grid to random search to Bayesian optimization will only require making minor modifications to these four parts.\n",
    "\n",
    "# Objective Function\n",
    "The objective function takes in hyperparameters and outputs a value representing a score. Traditionally in optimization, this is a score to minimize, but here our score will be the ROC AUC which of course we want to maximize. Later, when we get to Bayesian Optimization, we will have to use a value to minimize, so we can take  1−ROC AUC  as the score. What occurs in the middle of the objective function will vary according to the problem, but for this problem, we will use cross validation with the specified model hyperparameters to get the cross-validation ROC AUC. This score will then be used to select the best model hyperparameter values.\n",
    "\n",
    "In addition to returning the value to maximize, our objective function will return the hyperparameters and the iteration of the search. These results will let us go back and inspect what occurred during a search. The code below implements a simple objective function which we can use for both grid and random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validation ROC AUC was 0.70147.\n"
     ]
    }
   ],
   "source": [
    "score, params, iteration = objective(default_params, 1)\n",
    "\n",
    "print('The cross-validation ROC AUC was {:.5f}.'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain\n",
    "The domain, or search space, is all the possible values for all the hyperparameters that we want to search over. For random and grid search, the domain is a hyperparameter grid and usually takes the form of a dictionary with the keys being the hyperparameters and the values lists of values for each hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for GBM\n",
    "To see which settings we can tune, let's make a model and print it out. You can also refer to the LightGBM documentation for the description of all the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'silent': True,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a default model\n",
    "model = lgb.LGBMModel()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these we do not need to tune such as silent, objective, random_state, and n_jobs, and we will use early stopping to determine perhaps the most important hyperparameter, the number of individual learners trained, n_estimators (also referred to as num_boost_rounds or the number of iterations). Some of the hyperparameters do not need to be tuned if others are: for example, min_child_samples and min_child_weight both limit the complexity of individual decision trees by adjusting the minimum leaf observation requirements and therefore we will only adjust one. However, there are still many hyperparameters to optimize, and we will choose 10 to tune.\n",
    "\n",
    "Choosing a hyperparameter grid is probably the most difficult part of hyperparameter tuning: it's nearly impossible ahead of time to say which values of hyperparameters will work well and the optimal settings will depend on the dataset. Moreover, the hyperparameters have complex interactions with each other which means that just tuning one at a time doesn't work because when we start changing other hyperparameters that will affect the one we just tuned!\n",
    "\n",
    "If we have prior experience with a model, we might know where the best values for the hyperparameters typically lie, or what a good search space is. However, if we don't have much experience, we can simply define a large search space and hope that the best values are in there somewhere. Typically, when first using a method, I define a wide search space centered around the default values. Then, if I see that some values of hyperparameters tend to work better, I can concentrate the search around those values.\n",
    "\n",
    "A complete grid for the 10 hyperparameter is defined below. Each of the values in the dicionary must be a list, so we use list combined with range, np.linspace, and np.logspace to define the range of values for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "    'is_unbalance': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One aspect to note is that if boosting_type is goss, then we cannot use subsample (which refers to training on only a fraction of the rows in the training data, a technique known as stochastic gradient boosting). Therefore, we will need a line of logic in our algorithm that sets the subsample to 1.0 (which means use all the rows) if boosting_type=goss. As an example below, if we randomly select a set of hyperparameters, and the boosting type is \"goss\", then we set the subsample to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting type:  goss\n",
      "Subsample ratio:  1.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(50)\n",
    "\n",
    "# Randomly sample a boosting type\n",
    "boosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n",
    "\n",
    "# Set subsample depending on boosting type\n",
    "subsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n",
    "\n",
    "print('Boosting type: ', boosting_type)\n",
    "print('Subsample ratio: ', subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fa000bd208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Learning rate histogram\n",
    "plt.hist(param_grid['learning_rate'], bins = 20, color = 'r', edgecolor = 'k');\n",
    "plt.xlabel('Learning Rate', size = 14); plt.ylabel('Count', size = 14); plt.title('Learning Rate Distribution', size = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Domain\n",
    "The learning rate domain is from 0.005 to 0.5. Using a logarithmic uniform distribution allows us to create a domain where there are as many values from 0.005 to 0.05 as from 0.05 to 0.5. In a linear space, there would be far more values from 0.05 to 0.5 because this represents a larger distance in linear space but in logarithmic space each of these two intervals is the same width because they are multiples of 10 of each other. (Think about going from 1 to 10 and then from 10 to 100. On a logarithmic scale, these intervals are the same size, but on a linear scale the latter is 10 times the size of the former). In other words, a logarithmic uniform distribution lets us sample more evenly from a domain that varies over several orders of magnitude.\n",
    "\n",
    "If that's a little confusing, perhaps the graph above makes it clearer. We can also do a sanity check to make sure the spacing is correct by counting the number of values in each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 499 values between 0.005 and 0.05\n",
      "There are 499 values between 0.05 and 0.5\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "\n",
    "# Check number of values in each category\n",
    "for x in param_grid['learning_rate']:\n",
    "    # Check values\n",
    "    if x >= 0.005 and x < 0.05:\n",
    "        a += 1\n",
    "    elif x >= 0.05 and x < 0.5:\n",
    "        b += 1\n",
    "\n",
    "print('There are {} values between 0.005 and 0.05'.format(a))\n",
    "print('There are {} values between 0.05 and 0.5'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of a simple domain, the num_leaves is a uniform distribution. This means values are evenly spaced on a linear scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fa0112f2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of leaves domain\n",
    "plt.hist(param_grid['num_leaves'], color = 'm', edgecolor = 'k')\n",
    "plt.xlabel('Learning Number of Leaves', size = 14); plt.ylabel('Count', size = 14); plt.title('Number of Leaves Distribution', size = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.5 Algorithm for selecting next values\n",
    "Although we don't generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random.\n",
    "\n",
    "We will implement these algorithms very shortly, as soon as we cover the final part of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.6 Results History\n",
    "The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually uses the past results to decide on the next hyperparmeters to evaluate. Random and grid search are uninformed methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best!\n",
    "\n",
    "A dataframe is a useful data structure to hold the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes for random and grid search\n",
    "random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "\n",
    "grid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.7 Grid Search Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is best described as exhuastive guess and check. We have a problem: find the hyperparameters that result in the best cross validation score, and a set of values to try in the hyperparameter grid - the domain. The grid search method for finding the answer is to try all combinations of values in the domain and hope that the best combination is in the grid (in reality, we will never know if we found the best settings unless we have an infinite hyperparameter grid which would then require an infinite amount of time to run).\n",
    "\n",
    "Grid search suffers from one limiting problem: it is extremely computationally expensive because we have to perform cross validation with every single combination of hyperparameters in the grid! Let's see how many total hyperparameter settings there are in our simple little grid we developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2620800000000000 combinations\n"
     ]
    }
   ],
   "source": [
    "com = 1\n",
    "for x in param_grid.values():\n",
    "    com *= len(x)\n",
    "print('There are {} combinations'.format(com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until Kaggle upgrades the kernels to quantum computers, we are not going to be able to run evan a fraction of the combinations! Let's assume 100 seconds per evaluation and see how many years this would take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This would take 8310502283 years to finish.\n"
     ]
    }
   ],
   "source": [
    "print('This would take {:.0f} years to finish.'.format((100 * com) / (60 * 60 * 24 * 365)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we're going to need a better approach! Before we discuss alternatives, let's walk through how we would actually use this grid and evaluate all the hyperparameters.\n",
    "\n",
    "The code below shows the \"algorithm\" for grid search. First, we unpack the values in the hyperparameter grid (which is a Python dictionary) using the line keys, values = zip(*param_grid.items()). The key line is for v in itertools.product(*values) where we iterate through all the possible combinations of values in the hyperparameter grid one at a time. For each combination of values, we create a dictionary hyperparameters = dict(zip(keys, v)) and then pass these to the objective function defined earlier. The objective function returns the cross validation score from the hyperparameters which we record in the dataframe. This process is repeated for each and every combination of hyperparameter values. By using itertools.product (from this Stack Overflow Question and Answer), we create a generator rather than allocating a list of all possible combinations which would be far too large to hold in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, in grid search, we do not limit the number of evaluations. The number of evaluations is set by the total combinations in the hyperparameter grid (or the number of years we are willing to wait!). So the lines\n",
    "\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "would not be used in actual grid search. Here we will run grid search for 5 iterations just as an example. The results returned will show us the validation score (ROC AUC), the hyperparameters, and the iteration sorted by best performing combination of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best validation score was 0.71957\n",
      "\n",
      "The best hyperparameters were:\n",
      "{'boosting_type': 'gbdt',\n",
      " 'colsample_bytree': 0.6,\n",
      " 'is_unbalance': True,\n",
      " 'learning_rate': 0.004999999999999999,\n",
      " 'metric': 'auc',\n",
      " 'min_child_samples': 20,\n",
      " 'n_estimators': 132,\n",
      " 'num_leaves': 20,\n",
      " 'reg_alpha': 0.0,\n",
      " 'reg_lambda': 0.0,\n",
      " 'subsample': 0.5,\n",
      " 'subsample_for_bin': 20000,\n",
      " 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "grid_results = grid_search(param_grid)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(grid_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since we have the best hyperparameters, we can evaluate them on our \"test\" data (remember not the real test data)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from grid search scores 0.73374 ROC AUC on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "grid_search_params = grid_results.loc[0, 'params']\n",
    "\n",
    "# Create, train, test model\n",
    "model = lgb.LGBMClassifier(**grid_search_params, random_state=42)\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "preds = model.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that the model scores better on the test set than in cross validation. Usually the opposite happens (higher on cross validation than on test) because the model is tuned to the validation data. In this case, the better performance is probably due to small size of the test data and we get very lucky (although this probably does not translate to the actual competition data).\n",
    "\n",
    "To get a sense of how grid search works, we can look at the progression of hyperparameters that were evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the subsample and the is_unbalance because these are the only hyperparameters that change. In fact, the effect of changing these values is so small that validation scores literally did not change across runs (indicating this small of a change has no effect on the model). This is grid search trying every single value in the grid! No matter how small the increment between subsequent values of a hyperparameter, it will try them all. Clearly, we are going to need a more efficient approach if we want to find better hyperparameters in a reasonable amount of time.\n",
    "\n",
    "# Application\n",
    "If you want to run this on the entire dataset feel free to take these functions and put them in a script. However, I would advise against using grid search unless you have a very small hyperparameter grid because this is such as exhaustive method! Later, we will look at results from 1000 iterations of grid and random search run on the same small subset of data as we used above. I have not tried to run any form of grid search on the full data (and probably will not try this method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.0 Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search is surprisingly efficient compared to grid search. Although grid search will find the optimal value of hyperparameters (assuming they are in your grid) eventually, random search will usually find a \"close-enough\" value in far fewer iterations. This great paper explains why this is so: grid search spends too much time evaluating unpromising regions of the hyperparameter search space because it has to evaluate every single combination in the grid. Random search in contrast, does a better job of exploring the search space and therefore can usually find a good combination of hyperparameters in far fewer iterations.\n",
    "\n",
    "As this article lays out, random search should probably be the first hyperparameter optimization method tried because of its effectiveness. Even though it's an uninformed method (meaning it does not rely on past evaluation results), random search can still usually find better values than the default and is simple to run.\n",
    "\n",
    "Random search can also be thought of as an algorithm: randomly select the next set of hyperparameters from the grid! We can build a dictionary of hyperparameters by selecting one random value for each hyperparameter as follows (again accounting for subsampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'goss',\n",
       " 'colsample_bytree': 0.8222222222222222,\n",
       " 'is_unbalance': False,\n",
       " 'learning_rate': 0.027778881111994384,\n",
       " 'min_child_samples': 175,\n",
       " 'num_leaves': 88,\n",
       " 'reg_alpha': 0.8979591836734693,\n",
       " 'reg_lambda': 0.6122448979591836,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 220000}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(50)\n",
    "\n",
    "# Randomly sample from dictionary\n",
    "random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "# Deal with subsample ratio\n",
    "random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n",
    "\n",
    "random_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the random_search function. This takes the same general structure as grid_search except for the method used to select the next hyperparameter values. Moreover, random search is always run with a limit on the number of search iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # Keep searching until reach max evaluations\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best validation score was 0.72420\n",
      "\n",
      "The best hyperparameters were:\n",
      "{'boosting_type': 'dart',\n",
      " 'colsample_bytree': 0.9555555555555555,\n",
      " 'is_unbalance': True,\n",
      " 'learning_rate': 0.2797162853084688,\n",
      " 'metric': 'auc',\n",
      " 'min_child_samples': 365,\n",
      " 'n_estimators': 16,\n",
      " 'num_leaves': 50,\n",
      " 'reg_alpha': 0.6326530612244897,\n",
      " 'reg_lambda': 0.5510204081632653,\n",
      " 'subsample': 0.8434343434343434,\n",
      " 'subsample_for_bin': 220000,\n",
      " 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "random_results = random_search(param_grid)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(random_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the best random search model on the \"test\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from random search scores 0.72466 ROC AUC on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "random_search_params = random_results.loc[0, 'params']\n",
    "\n",
    "# Create, train, test model\n",
    "model = lgb.LGBMClassifier(**random_search_params, random_state = 42)\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "preds = model.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we see hyperparameter values that are all over the place, almost as if they had been selected at random! Random search will do a much better job than grid search of exploring the search domain (for the same number of iterations). If we have a limited time to evaluate hyperparameters, random search is a better option than grid search for exactly this reason.\n",
    "\n",
    "# 1.2.0 Stacking Random and Grid Search\n",
    "One option for a smarter implementation of hyperparameter tuning is to combine random search and grid search:\n",
    "\n",
    "Use random search with a large hyperparameter grid\n",
    "\n",
    "Use the results of random search to build a focused hyperparameter grid around the best performing hyperparameter values.\n",
    "\n",
    "Run grid search on the reduced hyperparameter grid.\n",
    "\n",
    "Repeat grid search on more focused grids until maximum computational/time budget is exceeded.\n",
    "\n",
    "In a later notebook (upcoming), we will look at methods that use the past evaluation results to pick the next hyperparameter values to try in the objective function. These methods (including Bayesian optimization) are essentially doing what we would do in the strategy outlined above: adjust the next values tried in the search from the previous results. The overall objective of these informed methods is to limit evaluations of the objective function by reasoning about the next values to try based on past evaluation results. These algorithms are therefore able to save time by evaluating more promising values of hyperparameters. This is a really cool topic and Bayesian optimization is fascinating so stay tuned for this upcoming notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.1 Next Steps\n",
    "We can now take these random and grid search functions and use them on the complete dataset or any dataset of our choosing. These search methods are very expensive, so expect the hyperparameter tuning to take a while.(I am currently running this script on a full set of features for 500 iterations and will make the results public when they are available. )\n",
    "\n",
    "For now, we will turn to implementing random and grid search on the reduced dataset for 1000 iterations just to compare the results (I took the code below and already ran it because even with the small dataset, it takes a very long time. The results are available as part of the data in this kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Extremely Important Note about Checking Files\n",
    "When you want to check the csv file, do not open it in Excel while the search is ongoing. This will cause a permission error in Python and the search will be terminated. Instead, you can view the end of the file by typing tail out_file.csv from Bash where out_file.csv is the name of the file being written to. There are also some text editors, such as notepad or Sublime Text, where you can open the results safely while the search is occurring. However, do not use Excel to open a file that is being written to in Python. This is a mistake I've made several times so you do not have to!\n",
    "\n",
    "Below is the code we need to run before the search. This creates the csv file, opens a connection, writes the header (column names), and then closes the connection. This will overwrite any information currently in the out_file, so change to a new file name every time you want to start a new search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create file and open connection\n",
    "out_file = 'random_search_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write column names\n",
    "headers = ['score', 'hyperparameters', 'iteration']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must slightly modify random_search and grid_search to write to this file every time. We do this by opening a connection, this time using the \"a\" option for append (the first time we used the \"w\" option for write) and writing a line with the desired information (which in this case is the cross validation score, the hyperparameters, and the number of the iteration). Then we close the connection until the function is called again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, out_file, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization. \n",
    "       Writes result of search to csv file every search iteration.\"\"\"\n",
    "    \n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(MAX_EVALS)))\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(random_params, i)\n",
    "        results.loc[i, :] = eval_results\n",
    "\n",
    "        # open connection (append option) and write results\n",
    "        of_connection = open(out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow(eval_results)\n",
    "        \n",
    "        # make sure to close connection\n",
    "        of_connection.close()\n",
    "        \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(param_grid, out_file, max_evals = MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\n",
    "       Writes result of search to csv file every search iteration.\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        # Select the hyperparameters\n",
    "        parameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        parameters['subsample'] = 1.0 if parameters['boosting_type'] == 'goss' else parameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(parameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # open connection (append option) and write results\n",
    "        of_connection = open(out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow(eval_results)\n",
    "        \n",
    "        # make sure to close connection\n",
    "        of_connection.close()\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-36819a9fba8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mof_connection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgrid_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-be0fa9e0de7a>\u001b[0m in \u001b[0;36mgrid_search\u001b[1;34m(param_grid, out_file, max_evals)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Evalute the hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0meval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c3d4e297cf94>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(hyperparameters, iteration)\u001b[0m\n\u001b[0;32m      9\u001b[0m      \u001b[1;31m# Perform n_folds cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n\u001b[1;32m---> 11\u001b[1;33m                         early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# results to retun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[0;32m    445\u001b[0m                                     \u001b[0mend_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[0mcvfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_agg_cv_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mhandlerFunction\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbooster\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboosters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m                 \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandlerFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1522\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1523\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_EVALS = 1000\n",
    "\n",
    "#Create file and open connection\n",
    "out_file = 'grid_search_trials_1000.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "#Write column names\n",
    "headers = ['score', 'hyperparameters', 'iteration']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()\n",
    "\n",
    "grid_results = grid_search(param_grid, out_file)\n",
    "\n",
    "\n",
    "# Create file and open connection\n",
    "out_file = 'random_search_trials_1000.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "#Write column names\n",
    "headers = ['score', 'hyperparameters', 'iteration']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()\n",
    "\n",
    "random_results = random_search(param_grid, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.3 Results on Limited Data\n",
    "We can examine 1000 search iterations of the above functions on the reduced dataset. Later, we can try the hyperparameters that worked the best for the small versions of the data on a complete dataset to see if the best hyperparameters translate when increasing the size of the data 30 times! The 1000 search iterations were not run in a kernel, although they might be able to finish (no guarantees) in the 12 hour time limit.\n",
    "\n",
    "First we can find out which method returned the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.3 Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
